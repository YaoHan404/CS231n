{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tensors are just like numpy arrays, but they can run on GPU   \n",
    "Pytorch Tensors API looks almost exactly like numpy   \n",
    "Here we fit a two-layer net using Pytorch Tensors:  \n",
    "\n",
    "手工计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass:compute predictions and loss\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    loss = (y_pred-y).pow(2).sum()  # L2（均方）损失函数 \n",
    "    \n",
    "    # Backward pass: manually compute gradients 反向传播的计算过程再复习复习\n",
    "    grad_y_pred = 2.0 * (y_pred-y)  # y_pred对loss求导\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)  # t()为转置\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用pytorch追踪计算图，自动求导    \n",
    "创建tensor时，参数 requires_grad = True, 使得PyTorch建立计算图自动求导  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(5):\n",
    "    # PyTorch keeps track of them in the graph\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    loss = (y_pred-y).pow(2).sum() \n",
    "    \n",
    "    loss.backward()  # 求得loss相对于requires_grad = True的参数的偏导\n",
    "    \n",
    "    with torch.no_grad():  # 下列代码的计算过程不在计算图中\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()  # 不置零的话grad会累加\n",
    "        w2.grad.zero_()  # 下划线意味着原地操作，直接改变w2，而不是返回计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch可以自己定义 autograd function  \n",
    "自定义autograd function需要给tensors 写 forward 和 backward 静态方法  \n",
    "ctx作为内部参数，存储forward backward中的数据，且要一一对应  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):  # 继承自Function类\n",
    "    @staticmethod  # 输入Variable，中间计算tensor，输出Variable\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min = 0)\n",
    "             \n",
    "    @staticmethod  # 均使用Variable\n",
    "    def backward(ctx, grad_y):\n",
    "        x, = ctx.saved_tensors  # 读取参数 \n",
    "        grad_input = grad_y.clone()\n",
    "        grad_input[x<0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "def my_relu(x):\n",
    "    return MyReLU.apply(x)  # Function父类中的apply函数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(5):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = my_relu(h)  # 使用自定义autograd function\n",
    "    y_pred = h_relu.mm(w2)  \n",
    "    loss = (y_pred-y).pow(2).sum() \n",
    "    \n",
    "    loss.backward()  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()  \n",
    "        w2.grad.zero_()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch.nn   \n",
    "更高级的封装   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(H, D_out))\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(5):\n",
    "    y_pred = model(x)  # 整个forward不用手工计算了，直接用torch.nn中封装好的函数，learnable weight在model中`\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)  # loss也使用封装好的\n",
    "    \n",
    "    loss.backward()  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for param in model.parameters():  # 遍历torch.nn构建的model的参数\n",
    "            param -= learning_rate * param.grad\n",
    "    model.zero_grad()   # 梯度置零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用optim优化器   \n",
    "weight的更新就不需要手动设置了，学习率也会自动改变  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(H, D_out))\n",
    "model.to(device)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 一般优先使用Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(5):\n",
    "    y_pred = model(x) \n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)  \n",
    "    \n",
    "    loss.backward()  \n",
    "    \n",
    "    optimizer.step()  # 更新参数\n",
    "    optimizer.zero_grad()  # 梯度置0，等价于model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继承torch.nn.Module模块来自定义模型，使得自定义的模型可以使用pytorch的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ParalleBlock(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(ParalleBlock, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out)\n",
    "        self.linear2 = torch.nn.Linear(D_in, D_out)\n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1(x)\n",
    "        h2 = self.linear2(x)\n",
    "        return (h1*h2).clamp(min=0)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "        ParalleBlock(D_in, H), # 使用自定义的torch.nn.Module\n",
    "        ParalleBlock(H, H),\n",
    "        torch.nn.Linear(H, D_out))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(5):\n",
    "    y_pred = model(x) \n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)  \n",
    "    loss.backward()  \n",
    "    optimizer.step()  # 更新参数\n",
    "    optimizer.zero_grad()  # 梯度置0，等价于model.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取 custom data\n",
    "重写Dataset，读数据、改数据    \n",
    "使用DataLoader，进行minibatching等    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 继承Dataset，并实现__getitem__() 和 __len__()\n",
    "class ReadMyDataset(Dataset):\n",
    "    def __init__(self,数据集路径,怎么划分数据集xx,xxx):\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 可以对原始数据进行一些处理，比如 Data augmentation、\n",
    "        # 图像裁剪平移翻转标准化、点云投影等等\n",
    "        return 这个index的sample，把sample的feature、label等返回\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 一共多少sample\n",
    "\n",
    "# 获得一个Dataset的实例\n",
    "my_dataset = ReadMyDataset(xxxxxxx)\n",
    "\n",
    "# 使用DataLoader对 dataset 进行minibatching、shuffling、multithreading..\n",
    "data_loader = DataLoader(\n",
    "                my_dataset, \n",
    "                batch_size=args.batch_size, \n",
    "                shuffle=, \n",
    "                num_workers=, \n",
    "                pin_memory=True, \n",
    "                sampler=train_sampler)\n",
    "\n",
    "# 使用数据，xx\\xxx试Dataset里__getitem__()返回的数据\n",
    "for xx,xxx, in data_loader: \n",
    "    # model forward\\backward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model\n",
    "迁移学习 微调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "resnet101 = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化\n",
    "#### Visdom\n",
    "功能和matplot差不多，但可以远程显示\n",
    "#### tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动态计算图\n",
    "Dynamic Computation Graphs\n",
    "每一次迭代都要重新构建计算图\n",
    "缺点是效率低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与其它框架的关系\n",
    "PyTorch适合研究、论文复现，吸收了Caffe2，底层为C++   \n",
    "\n",
    "Caffe2适合工业部署，底层为C++   \n",
    "\n",
    "Tensorflow研究、生产都可，且有谷歌的生态、TPU加持，但更新快、文档跟不上，Tensorflow2的诞生让Tensorflow1的代码出了大量问题   \n",
    "\n",
    "可以用微软的ONNX将A框架训练的模型用于B框架，所以可以将Pytorch的模型用Caffe运行     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
